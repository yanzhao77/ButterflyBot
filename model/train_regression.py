#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒå›å½’æ¨¡å‹é¢„æµ‹æœªæ¥æ”¶ç›Šç‡
ç”¨äºåŒå‘äº¤æ˜“ç­–ç•¥
"""

import sys
import os
import argparse
from datetime import datetime, timezone, timedelta
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.settings import SYMBOL, TIMEFRAME, BASE_PATH
from data.fetcher import fetch_ohlcv
from data.features import add_features

# é¢„æµ‹å‚æ•°
PREDICTION_WINDOW = 4  # é¢„æµ‹æœªæ¥4æ ¹Kçº¿çš„æ”¶ç›Šç‡
MIN_RETURN_THRESHOLD = 0.015  # æœ€å°æ”¶ç›Šç‡é˜ˆå€¼1.5%

def prepare_regression_data(df, prediction_window=PREDICTION_WINDOW):
    """
    å‡†å¤‡å›å½’è®­ç»ƒæ•°æ®
    
    ç›®æ ‡ï¼šé¢„æµ‹æœªæ¥Næ ¹Kçº¿çš„æ”¶ç›Šç‡
    """
    print(f"ğŸ“Š å‡†å¤‡å›å½’æ•°æ®...")
    print(f"  é¢„æµ‹çª—å£: {prediction_window}æ ¹Kçº¿")
    
    # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
    df['future_return'] = (df['close'].shift(-prediction_window) - df['close']) / df['close']
    
    # åˆ é™¤æ— æ³•è®¡ç®—æœªæ¥æ”¶ç›Šçš„è¡Œ
    df = df.dropna(subset=['future_return'])
    
    # ç»Ÿè®¡ä¿¡æ¯
    positive_samples = (df['future_return'] > MIN_RETURN_THRESHOLD).sum()
    negative_samples = (df['future_return'] < -MIN_RETURN_THRESHOLD).sum()
    neutral_samples = len(df) - positive_samples - negative_samples
    
    print(f"\n  æ ·æœ¬åˆ†å¸ƒ:")
    print(f"    ä¸Šæ¶¨æ ·æœ¬ (>{MIN_RETURN_THRESHOLD*100:.1f}%): {positive_samples} ({positive_samples/len(df)*100:.1f}%)")
    print(f"    ä¸‹è·Œæ ·æœ¬ (<-{MIN_RETURN_THRESHOLD*100:.1f}%): {negative_samples} ({negative_samples/len(df)*100:.1f}%)")
    print(f"    éœ‡è¡æ ·æœ¬: {neutral_samples} ({neutral_samples/len(df)*100:.1f}%)")
    
    print(f"\n  æ”¶ç›Šç‡ç»Ÿè®¡:")
    print(f"    å‡å€¼: {df['future_return'].mean()*100:.3f}%")
    print(f"    æ ‡å‡†å·®: {df['future_return'].std()*100:.3f}%")
    print(f"    æœ€å°å€¼: {df['future_return'].min()*100:.3f}%")
    print(f"    æœ€å¤§å€¼: {df['future_return'].max()*100:.3f}%")
    print(f"    ä¸­ä½æ•°: {df['future_return'].median()*100:.3f}%")
    
    return df

def train_regression_model(limit=35000, since_days=365):
    """è®­ç»ƒå›å½’æ¨¡å‹"""
    
    print("=" * 80)
    print("è®­ç»ƒå›å½’æ¨¡å‹ - é¢„æµ‹æœªæ¥æ”¶ç›Šç‡")
    print("=" * 80)
    print(f"\né…ç½®:")
    print(f"  äº¤æ˜“å¯¹: {SYMBOL}")
    print(f"  å‘¨æœŸ: {TIMEFRAME}")
    print(f"  æ•°æ®é‡: {limit}æ¡")
    print(f"  æ—¶é—´èŒƒå›´: æœ€è¿‘{since_days}å¤©")
    
    # è·å–æ•°æ®
    print(f"\nâ³ è·å–å†å²æ•°æ®...")
    since_date = datetime.now(timezone.utc) - timedelta(days=since_days)
    df = fetch_ohlcv(SYMBOL, TIMEFRAME, limit=limit, since=since_date)
    
    if df is None or df.empty:
        print("âŒ æ•°æ®è·å–å¤±è´¥")
        return None
    
    print(f"âœ… è·å– {len(df)} æ ¹Kçº¿")
    
    # æ·»åŠ ç‰¹å¾
    print(f"\nğŸ§© æ„å»ºç‰¹å¾...")
    df = add_features(df)
    
    if df.empty:
        print("âŒ ç‰¹å¾æ„å»ºå¤±è´¥")
        return None
    
    # å‡†å¤‡å›å½’æ•°æ®
    df = prepare_regression_data(df, PREDICTION_WINDOW)
    
    # ç‰¹å¾åˆ—
    feature_cols = [
        'open', 'high', 'low', 'close', 'volume',
        'return', 'log_return',
        'ma20', 'ma50', 'ma_diff',
        'rsi', 'macd', 'macd_signal', 'macd_hist',
        'volatility', 'volume_ratio'
    ]
    
    # æ£€æŸ¥ç‰¹å¾
    missing_cols = [col for col in feature_cols if col not in df.columns]
    if missing_cols:
        print(f"âŒ ç¼ºå°‘ç‰¹å¾: {missing_cols}")
        return None
    
    # åˆ é™¤åŒ…å«NaNçš„è¡Œ
    df = df.dropna(subset=feature_cols + ['future_return'])
    
    print(f"\nğŸ§© ç‰¹å¾ç»´åº¦: {len(feature_cols)} | æœ‰æ•ˆæ ·æœ¬: {len(df)}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X = df[feature_cols].values
    y = df['future_return'].values
    
    # åˆ†å‰²è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, shuffle=False
    )
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")
    
    # è®­ç»ƒLightGBMå›å½’æ¨¡å‹
    print(f"\nğŸš€ è®­ç»ƒLightGBMå›å½’æ¨¡å‹...")
    
    params = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'seed': 42
    }
    
    train_data = lgb.Dataset(X_train, label=y_train)
    test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=500,
        valid_sets=[test_data],
        callbacks=[
            lgb.early_stopping(stopping_rounds=50),
            lgb.log_evaluation(period=100)
        ]
    )
    
    # è¯„ä¼°æ¨¡å‹
    print(f"\nğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
    
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # è®­ç»ƒé›†æŒ‡æ ‡
    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
    train_mae = mean_absolute_error(y_train, y_pred_train)
    train_r2 = r2_score(y_train, y_pred_train)
    
    # æµ‹è¯•é›†æŒ‡æ ‡
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    test_r2 = r2_score(y_test, y_pred_test)
    
    print(f"\nè®­ç»ƒé›†:")
    print(f"  RMSE: {train_rmse*100:.3f}%")
    print(f"  MAE: {train_mae*100:.3f}%")
    print(f"  RÂ²: {train_r2:.4f}")
    
    print(f"\næµ‹è¯•é›†:")
    print(f"  RMSE: {test_rmse*100:.3f}%")
    print(f"  MAE: {test_mae*100:.3f}%")
    print(f"  RÂ²: {test_r2:.4f}")
    
    # æ–¹å‘å‡†ç¡®ç‡ï¼ˆé¢„æµ‹æ¶¨è·Œæ–¹å‘æ˜¯å¦æ­£ç¡®ï¼‰
    train_direction_acc = np.mean((y_pred_train > 0) == (y_train > 0))
    test_direction_acc = np.mean((y_pred_test > 0) == (y_test > 0))
    
    print(f"\næ–¹å‘å‡†ç¡®ç‡:")
    print(f"  è®­ç»ƒé›†: {train_direction_acc*100:.2f}%")
    print(f"  æµ‹è¯•é›†: {test_direction_acc*100:.2f}%")
    
    # ç‰¹å¾é‡è¦æ€§
    print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§ (Top 10):")
    importance = model.feature_importance(importance_type='gain')
    feature_importance = sorted(
        zip(feature_cols, importance),
        key=lambda x: x[1],
        reverse=True
    )
    for i, (feat, imp) in enumerate(feature_importance[:10], 1):
        print(f"  {i}. {feat}: {imp:.0f}")
    
    # ä¿å­˜æ¨¡å‹
    model_dir = BASE_PATH / 'models' / 'registry'
    model_dir.mkdir(parents=True, exist_ok=True)
    
    version = datetime.now().strftime('v%Y%m%d_%H%M')
    model_path = model_dir / f'{version}_regression.pkl'
    metadata_path = model_dir / f'{version}_regression.json'
    
    joblib.dump(model, model_path)
    
    metadata = {
        'version': version,
        'type': 'regression',
        'symbol': SYMBOL,
        'timeframe': TIMEFRAME,
        'prediction_window': PREDICTION_WINDOW,
        'train_samples': len(X_train),
        'test_samples': len(X_test),
        'test_rmse': float(test_rmse),
        'test_mae': float(test_mae),
        'test_r2': float(test_r2),
        'test_direction_acc': float(test_direction_acc),
        'features': feature_cols,
        'created_at': datetime.now(timezone.utc).isoformat()
    }
    
    import json
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    # æ›´æ–°latestæŒ‡é’ˆ
    latest_path = model_dir / 'latest_regression.txt'
    with open(latest_path, 'w') as f:
        f.write(version)
    
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜:")
    print(f"  ç‰ˆæœ¬: {version}")
    print(f"  è·¯å¾„: {model_path}")
    print(f"  å…ƒæ•°æ®: {metadata_path}")
    
    print(f"\nâœ… è®­ç»ƒæˆåŠŸ!")
    print(f"  æµ‹è¯•é›†RÂ²: {test_r2:.4f}")
    print(f"  æ–¹å‘å‡†ç¡®ç‡: {test_direction_acc*100:.2f}%")
    
    return model_path

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='è®­ç»ƒå›å½’æ¨¡å‹')
    parser.add_argument('--limit', type=int, default=35000, help='æ•°æ®é‡')
    parser.add_argument('--since_days', type=int, default=365, help='æ—¶é—´èŒƒå›´ï¼ˆå¤©ï¼‰')
    
    args = parser.parse_args()
    
    model_path = train_regression_model(
        limit=args.limit,
        since_days=args.since_days
    )
    
    sys.exit(0 if model_path else 1)
